# =============================================================================
#  Merge or cut file configure
#  $Id$
# =============================================================================

# ftp server pool
ftp.server1.ip=192.168.8.40
ftp.server1.port=21
ftp.server1.user=root
ftp.server1.passwd=1qaz2wsx

# merge task configure
merge.f_ps_a_u_day.source.file=/home/hadoop/export/f_ps_a_day_${hiveconf:yyyyMMdd}/
merge.f_ps_a_u_day.target.file=/home/haoddp/export/f_ps_a_day_${hiveconf:yyyyMMdd}.csv
merge.f_ps_a_u_day.source.code=
merge.f_ps_a_u_day.target.code=
merge.f_ps_a_u_day.head=A,B,C

# cut task configure
cut.f_ps_a_u_day.source.file=
cut.f_ps_a_u_day.target.file=
cut.f_ps_a_u_day.source.code=
cut.f_ps_a_u_day.target.code=
cut.f_ps_a_u_day.split.size=10m

# chmod task configure
chmod.f_ps_a_u_day.path=/data2/etl/export/
chmod.f_ps_a_u_day.prefix=f_ps_a_u_day_${hiveconf:yyyyMMdd}
chmod.f_ps_a_u_day.suffix=1.txt
chmod.f_ps_a_u_day.code=666

# ftp task configure
ftp.f_ps_a1_day.local.path=/home/hadoop/
ftp.f_ps_a1_day.remote.path=/home/hadoop/
ftp.f_ps_a1_day.prefix=
ftp.f_ps_a1_day.suffix=
ftp.f_ps_a1_day.server=server1

